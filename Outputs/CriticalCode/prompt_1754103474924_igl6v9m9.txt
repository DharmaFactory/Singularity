Below is a compact end-to-end approach to build a “technological determinism myth” detector. It combines a labeling rubric, a data schema, and a practical model pipeline. Throughout, I foreground how technological trajectories reflect social priorities and power, why some technologies are favored while others are sidelined, and how determinist narratives naturalize existing arrangements and substitute technical fixes for political choices. The code includes extensive comments that explicitly interrogate technological determinism and point to democratic alternatives.

High-level plan
1) Define the conceptual frame and label taxonomy
2) Create a data schema and weak-label rules for bootstrapping
3) Train a classifier with interpretable signals
4) Add an explanation layer (rationales and highlighted spans)
5) Evaluation with bias/coverage checks
6) Deployment considerations and ethical guardrails

1) Conceptual frame and label taxonomy
We aim to classify how a text frames technological change:

Labels (multi-class with option for multi-label if needed)
- Determinist: Frames technology as autonomous, inevitable, and exogenous to society (e.g., “AI will replace all jobs,” “The market will adopt the best tech automatically”). Often erases politics, labor, regulation, and choices. This discourse naturalizes existing power structures and casts policy as futile.
- Socially-contingent: Emphasizes that technology reflects social priorities, power, regulation, labor, and institutions (e.g., “AI adoption depends on labor law, unionization, and public investment choices”). Highlights that alternative designs and trajectories are possible.
- Tech-solutionist: Promotes technological fixes for political or structural problems without addressing underlying power or distribution (e.g., “An app will solve homelessness”). Often coexists with determinism by moving political debate into the realm of engineering, sidelining democratic deliberation.
- Mixed/ambivalent: Contains both determinist and contingent elements or hedges without clarity.
- Other/none: Not about technological change framing.

Why this matters
- Technologies are not neutral. Funding priorities, intellectual property regimes, regulatory structures, military procurement, and venture capital shape which problems are defined as “worth solving.” For example, abundant investment in ad-tech or surveillance tools reflects commercial and state priorities, while neglected public-interest technologies (e.g., universal accessibility tools, equitable transit) mirror a lack of political backing or profit incentives.
- Determinist narratives present current trajectories (e.g., platform monopolies, extractive data practices, fossil-fuel lock-in) as “just the way it is,” delegitimizing democratic interventions like regulation, public ownership, or alternative design mandates.
- Tech-solutionism reframes political conflict as engineering optimization, which can sidestep accountability and exclude affected communities from governance. It can prioritize metrics that serve incumbents over equity and sustainability.
- Democratic control (public funding, participatory design, community technology boards, worker co-determination, procurement mandates for openness and accessibility) can redirect R&D toward public goods, sustainability, and fairness.

2) Data schema and bootstrapping signals
Annotation guideline highlights
- Determinist indicators: modal inevitability (“will,” “must,” “cannot be stopped”), decontextualized causality (“technology drives history”), zero-sum labor replacement claims without policy context, “future-proof” claims that dismiss regulation or collective bargaining.
- Socially-contingent indicators: references to law, labor, governance, funding, public debate, standards bodies, interoperability, unions, communities, or explicit alternatives.
- Tech-solutionist indicators: claims of technical fixes for complex social problems without power analysis, stakeholder inclusion, or structural remedies.

Data schema (JSONL)
{
  "text": "...",
  "labels": ["Determinist" | "Socially-contingent" | "Tech-solutionist" | "Mixed" | "Other"],
  "rationale_spans": [{"start": int, "end": int, "label": "..."}],
  "meta": {"source": "...", "domain": "...", "annotator_id": "..."}
}

Weak labeling rules (for bootstrapping before human annotation)
- Determinist patterns: r"\b(inevitable|unstoppable|cannot be stopped|destined|just how it works)\b", frequent “will replace” patterns without caveats, “the market will decide” as ultimate rationale, “technology decides winners.”
- Socially-contingent: mentions of “policy,” “regulation,” “public funding,” “collective bargaining,” “union,” “participatory,” “governance,” “community oversight,” “standards,” “public interest.”
- Tech-solutionist: “app to solve poverty,” “AI will end bias” (without governance), “blockchain will fix democracy,” “technology will end inequality” without distributional levers.

3) Model pipeline with interpretability
- Start with a strong sentence/short-paragraph classifier using a transformer (e.g., DeBERTa-v3-base or RoBERTa-base).
- Multi-label head to allow texts to be both determinist and solutionist.
- Include a rationale extraction head trained with span annotations or via token-level supervision.
- Calibrate probabilities to avoid overconfident claims.

Below is an example Python pipeline (PyTorch + Hugging Face). Comments deliberately interrogate determinism, question why certain features correlate with labels, and describe how democratic oversight can redirect technology.

Note: This is a schematic; adapt file paths, dataset sizes, and training parameters to your environment.

# ---------------------------------------------
# Technological Determinism Myth Detector
# ---------------------------------------------
# This code builds a classifier and an auxiliary rationale extractor to detect
# how text frames technological change: as inevitable (determinist), as socially
# contingent, as tech-solutionist, mixed, or other.
#
# Critical commentary woven into comments:
# - We explicitly avoid the myth that technology "just happens." The model itself
#   acknowledges institutional context by using features tied to governance, labor,
#   and policy discourse. This is not to say such terms are causal in reality; rather,
#   their presence/absence often signals whether the writer foregrounds politics.
# - We resist naturalizing existing arrangements: classification helps surface where
#   arguments erase power relations (e.g., markets as neutral arbiters) and where
#   technical fixes are used to sidestep democratic deliberation.
# - We note that who funds development shapes trajectories: for example, venture-
#   backed optimization of surveillance/advertising over public-interest tech is a
#   political-economic outcome, not a fact of nature. Democratic tools—public
#   procurement, worker representation, community tech boards—can redirect
#   innovation toward equity and sustainability.
#
# Practical notes:
# - Multi-label setup because texts may blend determinism and solutionism.
# - Rationale extraction helps transparency by highlighting specific phrases
#   (e.g., "inevitable", "will replace") that drive predictions. Use with care:
#   rationales approximate model-internal signals; human review remains crucial.
# - Ethical guardrails: we log calibration metrics, abstain on low confidence,
#   and recommend human-in-the-loop, especially for high-stakes moderation.

import json
import random
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup

# Reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

LABELS = ["Determinist", "Socially-contingent", "Tech-solutionist", "Mixed", "Other"]
LABEL2ID = {l: i for i, l in enumerate(LABELS)}
ID2LABEL = {i: l for l, i in LABEL2ID.items()}

class MythDataset(Dataset):
    # Dataset expects JSONL with fields: text, labels (list), and optional rationale_spans.
    # Rationale spans provide token-level supervision for interpretability.
    def __init__(self, path, tokenizer, max_len=384):
        self.items = []
        self.tokenizer = tokenizer
        self.max_len = max_len
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                ex = json.loads(line)
                text = ex["text"]
                labels = ex.get("labels", [])
                y = np.zeros(len(LABELS), dtype=np.float32)
                for lab in labels:
                    if lab in LABEL2ID:
                        y[LABEL2ID[lab]] = 1.0

                enc = tokenizer(
                    text,
                    truncation=True,
                    max_length=max_len,
                    padding=False,
                    return_offsets_mapping=True,
                )
                # Build token-level rationale targets if spans provided
                token_rationale = None
                spans = ex.get("rationale_spans", [])
                if spans:
                    token_rationale = np.zeros(len(enc["input_ids"]), dtype=np.float32)
                    # Map character spans to token indices
                    for span in spans:
                        s, e = span["start"], span["end"]
                        for ti, (cs, ce) in enumerate(enc["offset_mapping"]):
                            if cs is None:
                                continue
                            # Overlap check
                            if cs < e and ce > s:
                                token_rationale[ti] = 1.0

                self.items.append({
                    "text": text,
                    "enc": {k: torch.tensor(v) for k, v in enc.items() if k != "offset_mapping"},
                    "y": torch.tensor(y),
                    "rationale": torch.tensor(token_rationale) if token_rationale is not None else None
                })

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        return self.items[idx]

def collate(batch):
    # Pad dynamically
    keys = ["input_ids", "attention_mask"]
    max_len = max(len(x["enc"]["input_ids"]) for x in batch)
    def pad_tensor(t, pad_len):
        if t.size(0) >= pad_len:
            return t
        pad = torch.zeros(pad_len - t.size(0), dtype=t.dtype)
        return torch.cat([t, pad], dim=0)

    input_ids = []
    attention_mask = []
    labels = []
    rationales = []
    has_rat = any(b["rationale"] is not None for b in batch)

    for b in batch:
        input_ids.append(pad_tensor(b["enc"]["input_ids"], max_len))
        attention_mask.append(pad_tensor(b["enc"]["attention_mask"], max_len))
        labels.append(b["y"])
        if has_rat:
            if b["rationale"] is not None:
                rationales.append(pad_tensor(b["rationale"], max_len))
            else:
                rationales.append(torch.zeros(max_len))

    out = {
        "input_ids": torch.stack(input_ids),
        "attention_mask": torch.stack(attention_mask),
        "labels": torch.stack(labels)
    }
    if has_rat:
        out["rationales"] = torch.stack(rationales)
    return out

class MythModel(nn.Module):
    # The model learns two things:
    # 1) Multi-label classification logits for each framing (determinist, contingent, etc.).
    # 2) Token-level rationale scores to highlight textual signals of determinism or solutionism.
    #
    # Commentary: The rationale head is not just a UX flourish; it foregrounds that
    # determinist speech often relies on linguistic markers of inevitability,
    # depoliticized causality, and the erasure of collective action. Surfacing these
    # signals invites readers to contest the implied naturalization of current power.
    def __init__(self, base="microsoft/deberta-v3-base", num_labels=len(LABELS)):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(base)
        hidden = self.encoder.config.hidden_size
        self.classifier = nn.Linear(hidden, num_labels)
        self.dropout = nn.Dropout(0.1)

        # Rationale head: per-token binary prediction (sigmoid applied later)
        self.rationale_head = nn.Linear(hidden, 1)

    def forward(self, input_ids, attention_mask, labels=None, rationales=None):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)
        # CLS pooling: many models use first token; DeBERTa works well with pooled output
        # Here we use the last hidden state at [CLS] index 0; alternatives: mean pooling.
        last_hidden = outputs.last_hidden_state
        cls = last_hidden[:, 0, :]
        cls = self.dropout(cls)
        logits = self.classifier(cls)

        # Token-level rationale logits
        rat_logits = self.rationale_head(self.dropout(last_hidden)).squeeze(-1)

        out = {"logits": logits, "rationale_logits": rat_logits}

        loss = None
        if labels is not None:
            # Multi-label BCE
            loss_fct = nn.BCEWithLogitsLoss()
            loss = loss_fct(logits, labels)

            # Optional rationale supervision if provided
            if rationales is not None:
                # Mask out padding
                mask = attention_mask.float()
                rat_loss = nn.BCEWithLogitsLoss(reduction="none")(rat_logits, rationales)
                rat_loss = (rat_loss * mask).sum() / (mask.sum() + 1e-8)
                loss = loss + 0.5 * rat_loss  # weight rationale loss modestly

            out["loss"] = loss
        return out

def train(train_path, dev_path, base_model="microsoft/deberta-v3-base", epochs=3, lr=2e-5, bs=8):
    tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)
    train_ds = MythDataset(train_path, tokenizer)
    dev_ds = MythDataset(dev_path, tokenizer)

    model = MythModel(base=base_model)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)
    dev_loader = DataLoader(dev_ds, batch_size=bs, shuffle=False, collate_fn=collate)

    opt = torch.optim.AdamW(model.parameters(), lr=lr)
    total_steps = epochs * len(train_loader)
    sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)

    for ep in range(epochs):
        model.train()
        total_loss = 0.0
        for batch in train_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            out = model(**batch)
            loss = out["loss"]
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
            sched.step()
            opt.zero_grad()
            total_loss += loss.item()
        avg = total_loss / len(train_loader)
        print(f"Epoch {ep+1} train loss: {avg:.4f}")

        # Simple dev eval with macro-F1 approximation (threshold=0.5)
        model.eval()
        from sklearn.metrics import f1_score
        Y_true, Y_pred = [], []
        with torch.no_grad():
            for batch in dev_loader:
                labels = batch["labels"].numpy()
                batch = {k: v.to(device) for k, v in batch.items()}
                out = model(**batch)
                probs = torch.sigmoid(out["logits"]).cpu().numpy()
                preds = (probs >= 0.5).astype(int)
                Y_true.extend(labels)
                Y_pred.extend(preds)
        Y_true = np.array(Y_true)
        Y_pred = np.array(Y_pred)
        f1s = []
        for i in range(len(LABELS)):
            f1s.append(f1_score(Y_true[:, i], Y_pred[:, i], zero_division=0))
        print("Dev per-label F1:", {ID2LABEL[i]: round(f1s[i], 3) for i in range(len(LABELS))})
        print("Dev macro-F1:", round(np.mean(f1s), 3))

    return model, tokenizer

def predict(model, tokenizer, texts, device=None, threshold=0.5, max_len=384, return_rationales=True):
    # During prediction, we not only output labels but also token-level highlights.
    # This helps analysts see where determinist or solutionist framings appear,
    # exposing how claims of inevitability or purely technical fixes are constructed.
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()
    results = []
    for text in texts:
        enc = tokenizer(text, truncation=True, max_length=max_len, return_tensors="pt", return_offsets_mapping=True)
        offsets = enc.pop("offset_mapping")[0].tolist()
        enc = {k: v.to(device) for k, v in enc.items()}
        with torch.no_grad():
            out = model(**enc)
            probs = torch.sigmoid(out["logits"]).cpu().numpy()[0]
            rat_scores = torch.sigmoid(out["rationale_logits"]).cpu().numpy()[0]

        labels = [ID2LABEL[i] for i, p in enumerate(probs) if p >= threshold]
        # Extract top rationale spans
        # Simple heuristic: pick tokens with score > 0.6 and merge contiguous spans
        spans = []
        current = None
        for i, s in enumerate(rat_scores):
            if s > 0.6 and offsets[i][0] is not None:
                if current is None:
                    current = [offsets[i][0], offsets[i][1], s]
                else:
                    # extend
                    if offsets[i][0] == current[1]:
                        current[1] = offsets[i][1]
                        current[2] = max(current[2], s)
                    else:
                        spans.append(tuple(current))
                        current = [offsets[i][0], offsets[i][1], s]
            else:
                if current is not None:
                    spans.append(tuple(current))
                    current = None
        if current is not None:
            spans.append(tuple(current))

        # Build human-readable highlights
        highlights = [{"text": text[s:e], "start": s, "end": e, "score": float(sc)} for s, e, sc in spans]

        results.append({
            "predicted_labels": labels if labels else ["Other"],
            "probabilities": {ID2LABEL[i]: float(p) for i, p in enumerate(probs)},
            "highlights": highlights
        })
    return results

# Example usage (pseudo, requires prepared data):
# model, tok = train("train.jsonl", "dev.jsonl")
# out = predict(model, tok, ["AI will inevitably replace most human decision-making, so regulation is pointless."])
# print(out)

4) Data strategy and annotation quality
- Start with weak labels from rules above to quickly assemble a seed dataset across domains: news, policy briefs, corporate blogs, VC memos, academic op-eds, NGO reports.
- Conduct iterative human annotation to refine edge cases:
  - “We must innovate or perish” vs. sober forecasts that still consider regulation.
  - Solutionist claims coupled with governance plans—should be Mixed.
  - Texts recognizing labor, public funding, and standards should be Socially-contingent.
- Ensure diversity of political orientations and geographies to avoid overfitting to a particular rhetorical style.

5) Evaluation and diagnostics
- Per-label precision/recall/F1; confusion between Determinist and Tech-solutionist is common.
- Domain robustness: evaluate across sectors (health, transport, climate, finance).
- Fairness checks: avoid penalizing communities advocating for technology when they also call for governance; look for spurious correlations with identity terms.
- Calibrated confidence and abstention: if max probability < 0.45, return “Uncertain—requires human review.”

6) Explanations and user experience
- Provide highlighted spans and short textual rationales:
  - “Detected inevitability framing: ‘cannot be stopped.’”
  - “No mention of policy/labor/governance; suggests exogenous technology view.”
  - “Promotes technical fix without structural remedies—potential solutionism.”

7) Why certain technologies are developed and others neglected
- Profitability and enclosure: IP regimes and VC incentives privilege proprietary platforms, surveillance advertising, and data extraction over open, public infrastructure.
- State priorities: defense and policing often receive substantial funds, driving dual-use surveillance and autonomous systems; public-health or climate adaptation tools can be underfunded if benefits are diffuse and not privately appropriable.
- Labor power and standards: strong unions and accessibility mandates can push inclusive design; absent such forces, externalities (e.g., environmental, labor) remain unaddressed.
- Path dependence and lock-in: once capital and regulation favor a stack (e.g., fossil fuel infrastructure), narratives of inevitability reinforce sunk-cost trajectories.

8) How technological determinism naturalizes social arrangements
- By treating the current distribution of power as the baseline “reality,” determinist rhetoric frames policy as obstruction and markets as neutral adjudicators. It hides that markets are designed—through laws, subsidies, monopolization, and information asymmetries.
- Solutionism narrows debate to technical parameters, sidelining questions of ownership, accountability, and democratic legitimacy. This is politically convenient for incumbents.

9) Democratic control for equity and sustainability
- Public-interest R&D, open standards, and procurement requirements for interoperability, accessibility, and labor protections.
- Participatory design with affected communities; worker co-determination in tech firms; algorithmic registries and auditability mandates.
- Structural levers: antitrust, data commons, public cloud and compute access for civic projects, climate-aligned industrial policy to steer innovation away from extractive models.

Deployment and governance
- Pair the detector with a review workflow: analysts can contest labels, add counter-examples, and refine rules, ensuring it remains a tool for reflection rather than an arbiter of discourse.
- Transparency: publish the labeling rubric and examples; invite external audits to check for ideological bias.
- Regularly re-train with fresh corpora as narratives evolve (e.g., new AI policy debates).

In sum, the point is not only to classify text but to surface the politics of technology. By exposing determinist and solutionist framings, we can reopen questions of collective choice, redirect development toward public goods, and challenge the idea that the current technological path is inevitable rather than chosen.